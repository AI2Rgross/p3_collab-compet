{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition using ddpg\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I use the DDPG baseline and some variation of it to train multiples agents to play tennis in a Unity ML-Agents environment. This project correspond to the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "## 1. Naive Approach\n",
    "### Start the Environment\n",
    "First, I imports necessary packages. If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next cell, check that the path for the environment is correct. Change the `file_name` parameter to match the location of the Unity environment that you downloaded before.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/raphe/Cours/Nanodegree_DRL/Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take the brain name  and set it as the default\n",
    "# Environments contain brains which are responsible for deciding the actions of their associated agents\n",
    "# We use 2 agents. Depending on the algorythm used to solve the environement\n",
    "# the agents may collaborate or not. But from a technical point of view one could say that they are indepandant\n",
    "# as they dont evolve in the same space.\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "As I stated in README.MD, the goal of the two agents is to bounce a ball over a net using a racket as long as possible. If the agent hits the ball over the net, it receives a reward of +0.1. But, if the agent lets the ball hit the ground or hit it oustide of the table, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket.\n",
    "Each agent has only two continuous actions available:\n",
    "- moving toward or away the net\n",
    "- jumping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, I use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "As one can see, the comportement of each agent and his performance are really bad. \n",
    "This is due to the fact that the agent has not yet learned how to play tennis. I use a random function to define the actions values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train one agent using DDPG  (Naive approach)\n",
    "In the next code cells, you can found the body of the training code I implemented for one agent to learn how to play tennis. It uses the Python API to control the agent and receive feedback from the environment.\n",
    "Here, I use the DDPG algorithm to control both racket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, env_info, state_size, action_size, brain_name,num_agents, agent,n_episodes=6000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations # get the current state (for each agent)\n",
    "        agent.reset() # reset the noise added to the state. Makes the training more robust.\n",
    "        score=np.zeros(num_agents) # initialize the score (for each agent)\n",
    "        for t in range(max_t):\n",
    "            actions = [agent.act(states[i],rate=0.9999) for i in range(num_agents)] # get action from each agent based on the current state\n",
    "            env_info = env.step(actions)[brain_name]  # update environment informations with the actions of each agent\n",
    "            next_states = env_info.vector_observations  # get next state (for each agent)\n",
    "            rewards = env_info.rewards  # get reward (for each agent)\n",
    "            score = score+rewards  # update the score for each agent\n",
    "            dones = env_info.local_done  # see if episode is finished\n",
    "            agent.step(states, actions, rewards, next_states, dones,num_agents,2) # add (state,actions,rewards,next_states) to replay buffer and train the actor-critic neural network\n",
    "            states = next_states # roll over the state to next time step\n",
    "            if any(dones):\n",
    "                break\n",
    "                \n",
    "        scores.append(np.max(score)) # add maximum score for to display the results later\n",
    "        scores_deque.append(np.max(score)) # add maximum score to windows for convergence check\n",
    "        print('\\rEpisode {}\\tAverage Score: {}\\tScore: {}'.format(i_episode, np.mean(scores_deque), np.max(score)), end=\"\")\n",
    "        if i_episode>100 and np.mean(scores_deque)>0.5: # check if the env is solved\n",
    "            print(\"envionment solved\")\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth') # save actor weights\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')# save critic weights\n",
    "            return scores\n",
    "        \n",
    "        if i_episode%100 ==0: # save intermediary solution every 100 iter\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth') # save actor weights\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth') # save critic weights\n",
    "   \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ddpg(env, env_info, state_size, action_size, brain_name,num_agents, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Episode 2546\tAverage Score: 0.5118000076711178\tScore: 2.600000038743019envionment solved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display results for naive approach training\n",
    "\n",
    "##### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the agent using multi agents DDPG\n",
    "In the next code cells, you can found the body of the training code I implemented for multi agents. It uses the Python API to control the agent and receive feedback from the environment.\n",
    "Here, each agent get informations about state and action of other agent. The actor-critic network for each agent is trained using the replay buffer (each of them has a copy). Therefore, agents can substitute each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = [ Agent(state_size=state_size, action_size=action_size, random_seed=10) for i in range(num_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ddpg(env, env_info, state_size, action_size, brain_name,num_agents, agent,n_episodes=6000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        states = env_info.vector_observations  # get the current state (for each agent)\n",
    "        for i in range(num_agents):\n",
    "            agent[i].reset() # reset the noise added to the state. Makes the training more robust.\n",
    "        score=np.zeros(num_agents)  # initialize the score (for each agent)\n",
    "        for t in range(max_t):\n",
    "           # actions = agent.act(states)\n",
    "            actions = [agent[i].act(states[i],rate=0.9999) for i in range(num_agents)] # get action from each agent based on the current state\n",
    "            env_info = env.step(actions)[brain_name]  # update environment informations with the actions of each agent\n",
    "            next_states = env_info.vector_observations  # get next state (for each agent)\n",
    "            rewards = env_info.rewards  # get reward (for each agent)\n",
    "            score = score+rewards  # update the score for each agent\n",
    "            dones = env_info.local_done  # see if episode finished\n",
    "            # agent[i].step: add (states,actions,rewards,next_states) to replay buffer of each agent \n",
    "            # train the actor critic Neural Network of each agent\n",
    "            # each agent share the same information\n",
    "            [agent[i].step(states, actions, rewards, next_states, dones,num_agents,2) for i in range(num_agents)]\n",
    "            states = next_states # roll over the state to next time step\n",
    "            if any(dones):\n",
    "                break\n",
    "                \n",
    "        scores.append(np.max(score)) # save the best agent score for display\n",
    "        scores_deque.append(np.max(score)) # save the best agent score into the windows for convergence checking\n",
    "        print('\\rEpisode {}\\tAverage Score: {}\\tScore: {}'.format(i_episode, np.mean(scores_deque), np.max(score)), end=\"\")\n",
    "        if i_episode>100 and np.mean(scores_deque)>0.5: # check if env is solved\n",
    "            print(\"envionment solved\")\n",
    "            [torch.save(agent[i].actor_local.state_dict(), 'multi_checkpoint_actor'+str(i)+'.pth') for i in range(num_agents)] # save actor weights for each agents\n",
    "            [torch.save(agent[i].critic_local.state_dict(), 'multi_checkpoint_critic'+str(i)+'.pth') for i in range(num_agents)] # save critic weights for each agents\n",
    "            return scores\n",
    "        \n",
    "        if i_episode%100 ==0:\n",
    "            [torch.save(agent[i].actor_local.state_dict(), 'multi_checkpoint_actor'+str(i)+'.pth') for i in range(num_agents)] # save actor weights for each agents\n",
    "            [torch.save(agent[i].critic_local.state_dict(), 'multi_checkpoint_critic'+str(i)+'.pth') for i in range(num_agents)] # save critic weights for each agents\n",
    "   \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1972\tAverage Score: 0.5029000074975193\tScore: 1.0000000149011612envionment solved\n"
     ]
    }
   ],
   "source": [
    "scores = multi_ddpg(env, env_info, state_size, action_size, brain_name,num_agents, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Episode 1972\tAverage Score: 0.5029000074975193\tScore: 1.0000000149011612envionment solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display results for training\n",
    "\n",
    "##### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test\n",
    "\n",
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_Test(env,brain_name,num_agents,agent,file_name_actor,file_name_critic,n_episodes=6000, max_t=1000):\n",
    "    \"\"\" Visualize agent using saved checkpoint. \"\"\"\n",
    "    # load saved weights\n",
    "    agent.actor_local.load_state_dict(torch.load(file_name_actor))\n",
    "    agent.critic_local.load_state_dict(torch.load(file_name_critic)) \n",
    "    scores = []  # list containing scores from each episode\n",
    " \n",
    "    for i_episode in range(1, n_episodes):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]  # environment informations\n",
    "        states = env_info.vector_observations \n",
    "        agent.reset()\n",
    "        score = np.zeros(num_agents) \n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states,rate=0.9999) # get action using the DDPG algorythme (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]  # update environment informations with the actions of each agent\n",
    "            next_states = env_info.vector_observations  # get next state (for each agent)\n",
    "            rewards = env_info.rewards  # get reward (for each agent)\n",
    "            dones = env_info.local_done  # see if episode finished\n",
    "            states = next_states  # roll over the state to next time step\n",
    "            score =score+ rewards  # update the score for each agent\n",
    "            if any(dones):  # see if episode has finished\n",
    "                break\n",
    "        print('\\rEpisode {}\\tAverage Score: {}, touch: {}'.format(i_episode, np.max(score),t), end=\"\")\n",
    "        scores.append(np.max(score)) # save the best score between both agents\n",
    "\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ddpg_Test(env,brain_name,num_agents,agent,file_name_actor,file_name_critic,n_episodes=6000, max_t=1000):\n",
    "    \"\"\" Visualize agent using saved checkpoint. \"\"\"\n",
    "    # load saved weights\n",
    "    for i in range(num_agents):\n",
    "        agent[i].actor_local.load_state_dict(torch.load(file_name_actor[i]))\n",
    "        agent[i].critic_local.load_state_dict(torch.load(file_name_critic[i])) \n",
    "        \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    score = 0\n",
    " \n",
    "    for i_episode in range(1, n_episodes):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        states = env_info.vector_observations \n",
    "        [agent[i].reset() for i in range(num_agents)]\n",
    "        score = np.zeros(num_agents) \n",
    "        for t in range(max_t):\n",
    "            actions = [agent[0].act(states[i],rate=0.9999) for i in range(num_agents)]\n",
    "            env_info = env.step(actions)[brain_name] \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            #agent.step(states, actions, rewards, next_states, dones,num_agents)\n",
    "            states = next_states\n",
    "            score =score+ rewards\n",
    "            if any(dones):\n",
    "                break\n",
    "        scores.append(np.max(score)) # save the best score between both agents\n",
    "    print('\\rEpisode {}\\tAverage Score: {}'.format(i_episode, np.max(score)), end=\"\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/raphe/Cours/Nanodegree_DRL/Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = [ Agent(state_size=state_size, action_size=action_size, random_seed=10) for i in range(num_agents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_actor=[]\n",
    "file_name_critic=[]\n",
    "for i in range(num_agents):\n",
    "    file_name_actor.append('checkpoint_actor'+str(0)+'.pth')\n",
    "    file_name_critic.append('checkpoint_critic'+str(0)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the weight, start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=Test(env,brain_name,num_agents,agent,file_name_actor,file_name_critic ,n_episodes=20,max_t=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Display score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
