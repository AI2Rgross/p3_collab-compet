\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{cite}
\usepackage{float}

\begin{document}
\title{Notes for the Collaboration and competition project}
\date{2018\\ November}
\author{Raphael Gross}
\maketitle




\section{Introduction}
This is the third project of the Udacity DRL course. My goal for this project is to get a better understanding of multi-agents environments using deep reinforcement learning by implementing some of last variation of Deep Deterministic Policy Gradient (DDPG) in pytorch. In order to achieve my goal, I describe briefly the main features of the DDPG methods, the Tennis environment used for testing the algorithm and finally, I show the results I obtained.

- ddpg: one agent, playing for both side.
- MA-DDPG: two agents with no shared memory.
- MA-DDPG: two agents with shared memory for the critic only.
- MA-DDPG: two agents with shared memory for critic and actor

\section{State of the Art for DDPG}

I used the DDPG algorithm to solve the Tennis environment. DDPG is a model-free, off-policy actor-critic algorithm that enables solving continuous action environment. 
DDPG is based on the deterministic policy gradient or DPG \cite{LHDWR14}.
DDPG has a parametrized actor function $\mu(s|{\theta}^{\mu})$ which specifies the current policy by deterministically mapping states to a specific action while the critic $Q(s,a)$ is learned using the Bellman equation.

DDPG uses some of the advantages of DQN such as sample replay and the target network.
Sample replay reduces correlations between sample. The replay buffer is constitued of tuples $(s,a,r,s')$  with $s$ the observations, $a$ actions, $r$ rewards and $s'$ the new states.
I stock the interaction between agents and the environment in the same replay buffer without distinction.

The DDPG model like DQN\cite{mnih2015humanlevel} uses a target network adapted to actor-critic and soft target updates for the network weights. We have an actor target $mu'$ and a critic target $Q'$. These two points increase the stability of the network and let it learn the action-value function. 

The principal issue in continuous action space is exploration. I define the exploration policy as the sum of the policy $\mu$ and some noise function $\mathcal{N}$ obtained using the Ornstein-Uhlenbeck process.


\section{Testing environment}
The environment I used to train and test my agent is called Tennis. In this environment, two agents play tennis while controlling rackets. The objective is to make as many passes as possible. To achieve this objective, they need to pass the ball over a net without letting the ball touch the ground or shooting the ball out of the boundary. It is therefore a collaborative environment. For each pass done succefully, the agent get a reward of +0.1. If the agent fault the ball a reward of -0.01 is attribued.

The observation space is composed of 24 variables and each action is determined by a vector of dimension 2 corresponding to going forward or bakward and jump. The action vector values are between -1.0 and 1.0.

For this project, I will use the multi-agent Unity environment. This environment is composed of 2 identical agents. Each of them is on one side of a tennis court. The task is episodic. After each episode, each agent gets a reward without discounting.  In order to solve the environment, the average maximum score needs to be greater than 0.5 over 100 consecutive episodes.

\begin{center}
\begin{figure}[H]
  \center
  \includegraphics[width=0.7\textwidth]{../PNG/env.png}
  \caption{Reacher environment}
  \label{fig:reacher_environment}
\end{figure}
\end{center}


\section{Results}
\subsection{DDPG model: naive approach} 
The DDPG method is composed of two networks one for the actor and one for the critic. I call this version the naive approche as one agent is playing with himself.

\subsubsection{Actor}
The actor network is composed of linear layers:
\begin{itemize}
\item fc1 : $nn.Linear(state\_size, fc1\_units)$
\item fc2 : $nn.Linear(fc1\_units, fc2\_units)$
\item fc3 : $nn.Linear(fc2\_units, action\_size)$
\end{itemize}

The weight for the first two hidden layers is initialized using a Xavier initialization.
The number of input and output nodes for the hidden layers is defined by the next parameters:

\begin{itemize}
\item $state\_size=24$
\item $fc1\_units=256$
\item $fc2\_units=128$
\item $action\_size=2$
\end{itemize}

In the forward pass, $fc1$ and $fc2$ are combined with a $ReLU$ rectifier. Finally, the policy is given after injecting $fc3$ into a tanh logistic sigmoid function.

\subsubsection{Critic}
The critic-network is composed of linear layers:

\begin{itemize}
\item $fc1 = nn.Linear(state\_size, fc1\_units)$
\item $fc2 = nn.Linear(fc1\_units+action\_size, fc2\_units)$
\item $fc3 = nn.Linear(fc2\_units, 1)$
\end{itemize}
The number of input and output nodes for the hidden layers is defined by the next parameters:

\begin{itemize}
\item $state\_size=24$
\item $fc1\_units=256$
\item $fc2\_units=128$
\item $action\_size=2$
\end{itemize}

The weights for the first two hidden layers are initialized using a Xavier initialization.
The architecture of the critic is a bit unusual. The critic is used to estimate $Q(s, a(\mu))$. But, the action contribution is added to the network only after the first hidden layer.

The authors of the DDPG method\cite{LHDWR14} decided to concatenate the output of the first layer with the action just before the second hidden layer. One possible reason for this choice is to reduce the impact of the critic gradient on the actor computation during the backpropagation.


\begin{itemize}
\item $x\_s   = F.relu(self.fc1(state))$
\item $x\_s\_a = torch.cat((x\_s, action), dim=1)$
\item $x\_s\_a = F.relu(self.fc2(x\_s\_a))$
\item $Q\_s\_a  = self.fc3(x\_s\_a)$
\end{itemize}

\subsection{Parameters}
The best result I obtained for the DDPG method was with the next parameters values:

\begin{itemize}
\item MEMORY SIZE = int(1e5) (replay buffer size)
\item BATCH SIZE = 128  (minibatch size)
\item GAMMA = 0.99  (discount factor)
\item TAU = 1e-3  (parameter value used for the soft update of the target weights)
\item LR\_ACTOR = 8e-5  (learning rate for actor)
\item LR\_CRITIC = 8e-5  (learning rate for critic)
\item UPDATE EVERY = 1  (how often the target network is updated)
\end{itemize}


\subsection{Score}
The maximum score is defined as the highest score between agents. The score is the sum of all the rewards an agent obtained during an episode. For the naive approach, we have technically one agent playing with himself. So we still have have two scores: one for "left side" and one for the "right side" of the net.

In Figure ~\ref{fig:ddpg}, I ploted the maximum score obtained by the agents at each episode. 
To solve the environment, the agent need to have an average score of 0.5 or above over 100 episodes. The algorythm took 1972 episodes to solve the environement with an average score for the last 100 episodes of: 0.5029000074975193.	The score for the last episode is 1.00 which corresponds to 10 sucessful passes. Also, the agent manage do reach at multiple times a score of 2.5 during the last 100 episodes.

\begin{center}
\begin{figure}[H]
  \center
  \includegraphics[width=0.7\textwidth]{../PNG/ddpg.png}
  \caption{Maximum score par episode}
  \label{fig:ddpg}
\end{figure}
\end{center}


\subsection{MA-DDPG model: no shared memory} 
In this section I use two independant agents. They do not share memory such as state, action or observation and both of them use the baseline DDPG algorythm.

In Figure ~\ref{fig:ma_ddpg}, I ploted the maximum score obtained by the agents at each episode. 
The algorythm took 2165 episodes to solve the environement with an average score for the last 100 episodes of: 0.514.	The score for the last episode is 2.60 which corresponds to 26 sucessful exchanges. Also, the agents manage to reach at multiple times a score of 2.6 during the last few episodes.

\begin{center}
\begin{figure}[H]
  \center
  \includegraphics[width=0.7\textwidth]{../PNG/ma_ddpg.png}
  \caption{Maximum score par episode}
  \label{fig:ma_ddpg}
\end{figure}
\end{center}


\subsection{Future improvement}
For now, I only covered the DDPG\cite{LillicrapHPHETS15} algorithm. But, my ambition is to finish the PPO method\cite{ClaveraRS0AA18} I started to implement for continuous action and also look into A3C\cite{MnihBMGLHSK16}, A2C\cite{MnihBMGLHSK16} and D4PG\cite{Barth2018}. i will also try to solve other environment such as the Crawler environment.
\bibliography{bib}
\bibliographystyle{plain}

\end{document}


